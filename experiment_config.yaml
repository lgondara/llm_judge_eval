# Experiment Configuration for LLM-as-Judge Scoring Strategies
# Research Question: Which scoring strategy produces best downstream model performance?

experiment:
  name: "judge_scoring_strategies_v1"
  seed: 42
  output_dir: "./outputs"

# Data Configuration
data:
  source: "lmsys/lmsys-chat-1m"  # or "HuggingFaceH4/ultrafeedback_binarized"
  domain_filter: "code"  # Options: "code", "math", "general"
  max_samples: 50000
  train_split: 0.9
  cache_dir: "./cache"

# Judge Model Configuration
judge:
  model_name: "meta-llama/Llama-3.1-70B-Instruct"  # Primary judge
  # Alternatives for ablation:
  # - "meta-llama/Llama-3.1-8B-Instruct"
  # - "mistralai/Mistral-7B-Instruct-v0.3"
  device_map: "auto"
  torch_dtype: "bfloat16"
  max_new_tokens: 256
  temperature: 0.0  # Deterministic scoring

# Scoring Strategies to Evaluate
scoring_strategies:
  - name: "binary"
    type: "pointwise"
    scale: [0, 1]
    
  - name: "likert_5"
    type: "pointwise"
    scale: [1, 2, 3, 4, 5]
    
  - name: "numeric_100"
    type: "pointwise"
    scale: [0, 100]
    
  - name: "anchored"
    type: "pairwise"
    anchor_strategy: "median"  # Options: "median", "random", "fixed"
    scale: [-2, -1, 0, 1, 2]  # Much worse, worse, tie, better, much better

# Curation Configuration
curation:
  retention_rates: [0.10, 0.25, 0.50, 0.75, 1.00]
  selection_method: "percentile"  # Top-k percentile

# Finetuning Configuration
training:
  base_model: "meta-llama/Llama-3.1-8B-Instruct"
  # LoRA Configuration
  lora:
    r: 16
    lora_alpha: 32
    lora_dropout: 0.05
    target_modules: ["q_proj", "k_proj", "v_proj", "o_proj"]
  # Training Hyperparameters
  num_epochs: 3
  batch_size: 4
  gradient_accumulation_steps: 4
  learning_rate: 2.0e-5
  warmup_ratio: 0.1
  max_seq_length: 2048
  num_replicates: 3  # Different seeds per configuration

# Evaluation Configuration
evaluation:
  benchmarks:
    - name: "humaneval"
      metric: "pass@1"
    - name: "mbpp"
      metric: "pass@1"
    # Optional for generalization check:
    # - name: "mt_bench"
    #   metric: "score"
  num_samples_per_task: 1  # For pass@1
